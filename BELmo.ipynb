{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BELmo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO5/nl5edvL8KJbMzzzDXQN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexanderBelfort/MDPRKT/blob/main/BELmo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxmuEvEBpjbk"
      },
      "source": [
        "## Step 1 - Setup all necesarry libraries\r\n",
        "\r\n",
        "* semantic analysis / data extraction modules: GENSIM, spaCy, more\r\n",
        "* plotting tools for visualization: pyLDAvis ver==2.1.2\r\n",
        "* download ELMo's BiLSTM: ver==1.0.0\r\n",
        "* download BERT\r\n",
        "* create Data directory where user will be uploading their data\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_tN6zznA8oE"
      },
      "source": [
        "#@title ⠀ {display-mode: \"form\"}\r\n",
        "\r\n",
        "\r\n",
        "# install al models\r\n",
        "%pip install allennlp==1.0.0 allennlp-models==1.0.0\r\n",
        "%pip install transformers\r\n",
        "%pip install spacy\r\n",
        "%pip install pyLDAvis==2.1.2\r\n",
        "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/decomposable-attention-elmo-2020.04.09.tar.gz\")\r\n",
        "\r\n",
        "!mkdir data\r\n",
        "!cd data\r\n",
        "\r\n",
        "# Gensim\r\n",
        "import gensim\r\n",
        "import gensim.corpora as corpora\r\n",
        "from gensim.utils import simple_preprocess\r\n",
        "from gensim.models import CoherenceModel\r\n",
        "\r\n",
        "# spacy for lemmatization\r\n",
        "import spacy\r\n",
        "\r\n",
        "# Plotting tools\r\n",
        "import pyLDAvis\r\n",
        "import pyLDAvis.gensim \r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import re\r\n",
        "import string\r\n",
        "import numpy as np\r\n",
        "from wordcloud import WordCloud, STOPWORDS\r\n",
        "import pandas as pd\r\n",
        "from pprint import pprint\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "#ELMo\r\n",
        "from allennlp.predictors.predictor import Predictor\r\n",
        "import allennlp_models.tagging\r\n",
        "\r\n",
        "\r\n",
        "#BERT\r\n",
        "from transformers import pipeline\r\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\r\n",
        "from transformers import AutoConfig, AutoModel\r\n",
        "\r\n",
        "\r\n",
        "# Enable logging for gensim - optional\r\n",
        "import logging\r\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\r\n",
        "\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from IPython.display import display, clear_output\r\n",
        "from IPython.utils import io\r\n",
        "import ipywidgets as widgets\r\n",
        "from ipywidgets import FileUpload\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQKgU8dkp0F7"
      },
      "source": [
        "## Step 2 - Upload a transcript\r\n",
        "* Allowed extensions: .txt\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnDDd3Y2Jmdw"
      },
      "source": [
        "#@title ⠀ {display-mode: \"form\"}\r\n",
        "\r\n",
        "\r\n",
        "upload = FileUpload(accept='.txt', multiple=True)\r\n",
        "display(upload)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A-3DvVlqSWi"
      },
      "source": [
        "## Step 3 - Generate a WordCloud\r\n",
        "* generating a wordcloud can help us identify core themes\r\n",
        "* the wordcloud will be saved in the directory for future reference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAnooYGVMMAm"
      },
      "source": [
        "#@title ⠀ {display-mode: \"form\"}\r\n",
        "# create a new file called uploaded_file that we will use for our wordcloud\r\n",
        "with open('data/uploaded_file.txt', 'wb') as output_file: \r\n",
        "\r\n",
        "    for uploaded_filename in upload.value:\r\n",
        "        content = upload.value[uploaded_filename]['content']   \r\n",
        "        output_file.write(content) \r\n",
        "      \r\n",
        "\r\n",
        "# read the file content\r\n",
        "file_content=open (\"data/uploaded_file.txt\").read()\r\n",
        "\r\n",
        "# create an extra stopwords list where we will add\r\n",
        "# all irrelevant words \r\n",
        "# for example researcher and patient (R1, R2, P1, P2) are dominant words but highly irrelevant\r\n",
        "stoppwords = [\"R1\", \"R2\", \"P1\", \"P2\", \"ve\", \"Okay\", \"Yeah\", \"right\", \"think\", \"will\", \"now\", \"go\", \"got\", \"yes\", \"fine\", \"say\", \"going\"]\r\n",
        "\r\n",
        "\r\n",
        "def random_color_func(word=None, \\\r\n",
        "                      font_size=None, \\\r\n",
        "                      position=None, \\\r\n",
        "                      orientation=None, \\\r\n",
        "                      font_path=None, \\\r\n",
        "                      random_state=None):\r\n",
        "  \r\n",
        "    h = int(360.0 * 45.0 / 255.0)\r\n",
        "    s = int(100.0 * 255.0 / 255.0)\r\n",
        "    l = int(100.0 * float(random_state.randint(60, 120)) / 255.0)\r\n",
        "\r\n",
        "    return \"hsl({}, {}%, {}%)\".format(h, s, l)\r\n",
        "\r\n",
        "# plot the wordcloud and save it to the directory \r\n",
        "wordcloud = WordCloud(\r\n",
        "                            stopwords = stoppwords + list(STOPWORDS),\r\n",
        "                            background_color = 'white',\r\n",
        "                            color_func = random_color_func\r\n",
        "                      \r\n",
        "                            ).generate(file_content)\r\n",
        "plt.figure(figsize = (8, 3), facecolor = None)\r\n",
        "plt.imshow(wordcloud)\r\n",
        "plt.axis('off')\r\n",
        "plt.tight_layout(pad = 0) \r\n",
        "plt.savefig('word_cloud.png')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgOboZiPqfIL"
      },
      "source": [
        "## Step 3 - Generate a Topic Model\r\n",
        "* The model is finetuned\r\n",
        "* The model can be further improved if coherence is too low\r\n",
        "* N of topics set to 10\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erTgQ4J_O3nE"
      },
      "source": [
        "#@title ⠀ {display-mode: \"form\"}\r\n",
        "\r\n",
        "# NLTK Stop words\r\n",
        "from nltk.corpus import stopwords\r\n",
        "stop_words = stopwords.words('english')\r\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\r\n",
        "\r\n",
        "# Import Dataset\r\n",
        "df = pd.read_csv('/content/data/uploaded_file.txt', sep='\\t')\r\n",
        "#print(df.target_names.unique())\r\n",
        "#df.head()\r\n",
        "\r\n",
        "# testing\r\n",
        "\r\n",
        "#print(df.columns)\r\n",
        "# returns R - Researcher and P - Participant which are not great names for columns\r\n",
        "# df.columns[0]\r\n",
        "# needs renaming to CONTENT\r\n",
        "col_name = df.columns[0]\r\n",
        "df=df.rename(columns = {col_name:'content'})\r\n",
        "# testing\r\n",
        "# df.columns[0]\r\n",
        "# returns content now\r\n",
        "\r\n",
        "\r\n",
        "# DATA CLEANING STEP\r\n",
        "#\r\n",
        "#\r\n",
        "# Convert to list\r\n",
        "data = df.content.values.tolist()\r\n",
        "\r\n",
        "# Remove new line characters\r\n",
        "data = [re.sub('\\s+', ' ', token) for token in data]\r\n",
        "\r\n",
        "# Remove distracting single quotes\r\n",
        "data = [re.sub(\"\\'\", \"\", token) for token in data]\r\n",
        "\r\n",
        "# remove text in square brackets like [surgeon], [name]\r\n",
        "data = [re.sub('\\[.*?\\]', '', token) for token in data]\r\n",
        "\r\n",
        "# remove numbers\r\n",
        "data = [re.sub('\\w*\\d\\w*', '', token) for token in data]\r\n",
        "\r\n",
        "# remove punctuation\r\n",
        "# might not be necessary now / we can do it later\r\n",
        "# data = [re.sub('[%s]' % re.escape(string.punctuation), '', token) for token in data]\r\n",
        "\r\n",
        "# all to lower\r\n",
        "data = [word.lower() for word in data]\r\n",
        "\r\n",
        "# filter out short tokens like P and R and i\r\n",
        "# neccessary?\r\n",
        "#data = [re.sub(r'\\b\\w{1,1}\\b', '', token) for token in data]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# Let’s tokenize each sentence into a list of words\r\n",
        "# removing punctuations and unnecessary characters altogether\r\n",
        "# on this step\r\n",
        "\r\n",
        "def sent_to_words(sentences):\r\n",
        "    for sentence in sentences:\r\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\r\n",
        "\r\n",
        "data_words = list(sent_to_words(data))\r\n",
        "\r\n",
        "\r\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\r\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \r\n",
        "\r\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\r\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\r\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\r\n",
        "\r\n",
        "# See trigram example\r\n",
        "# print(trigram_mod[bigram_mod[data_words[0]]])\r\n",
        "\r\n",
        "# this does not work so good as the words\r\n",
        "# gallblader surgery\r\n",
        "# laporscopic cholefcystemy\r\n",
        "# or any of that kind do not really appear often\r\n",
        "# maybe mentioned once\r\n",
        "\r\n",
        "def remove_stopwords(texts):\r\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\r\n",
        "\r\n",
        "def make_bigrams(texts):\r\n",
        "    return [bigram_mod[doc] for doc in texts]\r\n",
        "\r\n",
        "def make_trigrams(texts):\r\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\r\n",
        "\r\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\r\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\r\n",
        "    texts_out = []\r\n",
        "    for sent in texts:\r\n",
        "        doc = nlp(\" \".join(sent)) \r\n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\r\n",
        "    return texts_out\r\n",
        "\r\n",
        "data_words_nostops = remove_stopwords(data_words)\r\n",
        "\r\n",
        "# Form Bigrams\r\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\r\n",
        "\r\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\r\n",
        "# python3 -m spacy download en\r\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\r\n",
        "\r\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\r\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\r\n",
        "\r\n",
        "# print(data_lemmatized[:5])\r\n",
        "# # # # # # # # # #\r\n",
        "# # # # # # # # # \r\n",
        "## # # # # # \r\n",
        "##\r\n",
        "#\r\n",
        "\r\n",
        "id2word = corpora.Dictionary(data_lemmatized)\r\n",
        "\r\n",
        "# Create Corpus\r\n",
        "texts = data_lemmatized\r\n",
        "\r\n",
        "# Term Document Frequency\r\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\r\n",
        "\r\n",
        "# View\r\n",
        "# print(corpus[:5])\r\n",
        "\r\n",
        "# words really are not repeated that often :/\r\n",
        "# maximum of two repetitions in the first sentence\r\n",
        "# that is why I added would 5 times at index 1\r\n",
        "# print(corpus[1])\r\n",
        "\r\n",
        "# Human readable format of corpus (term-frequency)\r\n",
        "# [[(id2word[id], freq) for id, freq in cp] for cp in corpus[:2]]\r\n",
        "\r\n",
        "# # # # # # # # # # # # # # # # #\r\n",
        "# Build LDA model\r\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\r\n",
        "                                           id2word=id2word,\r\n",
        "                                           num_topics=10, \r\n",
        "                                           random_state=111, #fine tuned\r\n",
        "                                           update_every=1,\r\n",
        "                                           chunksize=100,\r\n",
        "                                           passes=10,\r\n",
        "                                           alpha=0.1, #fine tuned\r\n",
        "                                           per_word_topics=True)\r\n",
        "\r\n",
        "# Print the Keyword in the 10 topics\r\n",
        "print('\\n\\nFormed Clusters and the key terms that define them:\\n\\n\\n')\r\n",
        "pprint(lda_model.print_topics())\r\n",
        "doc_lda = lda_model[corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrUpbh6Equut"
      },
      "source": [
        "It is good to check the perprexity and coherence scores with the fine-tuned model to check if further fine-tuning is needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqO0jRCpPWoU"
      },
      "source": [
        "#@title ⠀ {display-mode: \"form\"}\r\n",
        "\r\n",
        "\r\n",
        "# visualize and check perplexity and coherence score\r\n",
        "# Compute Perplexity\r\n",
        "print('The overall coherence score of a topic is the average of the distances between words.')\r\n",
        "print('.3 is bad\\n.4 is low\\n.55 and above is good')\r\n",
        "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\r\n",
        "\r\n",
        "# Compute Coherence Score\r\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\r\n",
        "coherence_lda = coherence_model_lda.get_coherence()\r\n",
        "print('\\nCoherence Score: ', coherence_lda)\r\n",
        "\r\n",
        "# Visualize the topics\r\n",
        "pyLDAvis.enable_notebook()\r\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\r\n",
        "vis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoKaDsrBrDRT"
      },
      "source": [
        "If coherence is too low, generate a number of models and plot them\r\n",
        "* check which model works the best\r\n",
        "* use that model's N of topics\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52sCX0Slf4sY"
      },
      "source": [
        "#@title ⠀ {display-mode: \"form\"}\r\n",
        "\r\n",
        "\r\n",
        "# find optimal number of topics for highest coherence\r\n",
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\r\n",
        "    \"\"\"\r\n",
        "    Compute c_v coherence for various number of topics\r\n",
        "\r\n",
        "    Parameters:\r\n",
        "    ----------\r\n",
        "    dictionary : Gensim dictionary\r\n",
        "    corpus : Gensim corpus\r\n",
        "    texts : List of input texts\r\n",
        "    limit : Max num of topics\r\n",
        "\r\n",
        "    Returns:\r\n",
        "    -------\r\n",
        "    model_list : List of LDA topic models\r\n",
        "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\r\n",
        "    \"\"\"\r\n",
        "    coherence_values = []\r\n",
        "    model_list = []\r\n",
        "    for num_topics in range(start, limit, step):\r\n",
        "        model=gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\r\n",
        "        model_list.append(model)\r\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\r\n",
        "        coherence_values.append(coherencemodel.get_coherence())\r\n",
        "\r\n",
        "    return model_list, coherence_values\r\n",
        "\r\n",
        "    #coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\r\n",
        "\r\n",
        "\r\n",
        "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=40, step=6)\r\n",
        "# Show graph\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "limit=40; start=2; step=6;\r\n",
        "x = range(start, limit, step)\r\n",
        "plt.plot(x, coherence_values)\r\n",
        "plt.xlabel(\"Num Topics\")\r\n",
        "plt.ylabel(\"Coherence score\")\r\n",
        "plt.legend((\"coherence_values\"), loc='best')\r\n",
        "plt.savefig('lda_models.png')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSuRKhLlrLuL"
      },
      "source": [
        "## Step 4 - Ask Questions\r\n",
        "* choose questions that are relevant to the word cloud / topics\r\n",
        "* starting with Randomisation\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2on1Acal8Is"
      },
      "source": [
        "#@title ⠀ {display-mode: \"form\"}\r\n",
        "\r\n",
        "\r\n",
        "# best performing model for our QA\r\n",
        "name = 'distilbert-base-cased-distilled-squad'\r\n",
        "\r\n",
        "tokenizer = AutoTokenizer.from_pretrained(name,)\r\n",
        "\r\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(name)\r\n",
        "\r\n",
        "# init inference pipeline \r\n",
        "nlp = pipeline('question-answering', model = model, tokenizer = tokenizer)\r\n",
        "\r\n",
        "print('Due to size of context, answers take time to load.\\n\\n')\r\n",
        "\r\n",
        "w = widgets.Dropdown(\r\n",
        "    options=['What are you randomised for?', 'What does the computer decide?', 'Who makes the decision?', 'What is randomisation?'],\r\n",
        "    value=None,\r\n",
        "    description='Q1:',\r\n",
        ")\r\n",
        "\r\n",
        "\r\n",
        "def on_change(change):\r\n",
        "    if change['type'] == 'change' and change['name'] == 'value':\r\n",
        "        surgery_question = change['new']\r\n",
        "        \r\n",
        "        def answer_que(questt, anss):\r\n",
        "          print(nlp({\r\n",
        "          'question': questt,\r\n",
        "          'context': anss\r\n",
        "      }))\r\n",
        "             \r\n",
        "        def transcriptName():\r\n",
        "\r\n",
        "          with open('data/uploaded_file.txt', 'r') as myfile: \r\n",
        "\r\n",
        "            next(myfile)\r\n",
        "            data=myfile.read().replace('\\n', '')\r\n",
        "\r\n",
        "            randomisation_que = answer_que(surgery_question, data)\r\n",
        "        print('Computing...')\r\n",
        "\r\n",
        "        transcriptName()\r\n",
        "\r\n",
        "\r\n",
        "w.observe(on_change)\r\n",
        "\r\n",
        "display(w)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "x = widgets.Dropdown(\r\n",
        "    options=['Does surgery cure pain?', 'What is the best way to treat gallstones? ', 'What is the reason for doing the trial?' , 'What is the best treatment?', 'Is one treatment better than the other?', 'Is surgery the way to go?', 'Is operation the way to go?', 'Does having an operation cure?'],\r\n",
        "    value=None,\r\n",
        "    description='Q2:',\r\n",
        ")\r\n",
        "\r\n",
        "\r\n",
        "def on_change(change):\r\n",
        "    if change['type'] == 'change' and change['name'] == 'value':\r\n",
        "        surgery_question = change['new']\r\n",
        "        \r\n",
        "        def answer_que(questt, anss):\r\n",
        "          print(nlp({\r\n",
        "          'question': questt,\r\n",
        "          'context': anss\r\n",
        "      }))\r\n",
        "             \r\n",
        "        def transcriptName():\r\n",
        "\r\n",
        "          with open('data/uploaded_file.txt', 'r') as myfile: \r\n",
        "\r\n",
        "            next(myfile)\r\n",
        "            data=myfile.read().replace('\\n', '')\r\n",
        "\r\n",
        "            randomisation_que = answer_que(surgery_question, data)\r\n",
        "        \r\n",
        "        print('Computing...')\r\n",
        "        transcriptName()\r\n",
        "\r\n",
        "\r\n",
        "x.observe(on_change)\r\n",
        "\r\n",
        "display(x)\r\n",
        "\r\n",
        "\r\n",
        "y = widgets.Dropdown(\r\n",
        "    options=['What is medical management?', 'What is conservative management?', 'What is the surgery called?', 'What is the surgery?'],\r\n",
        "    value=None,\r\n",
        "    description='Q3:',\r\n",
        ")\r\n",
        "\r\n",
        "\r\n",
        "def on_change(change):\r\n",
        "    if change['type'] == 'change' and change['name'] == 'value':\r\n",
        "        surgery_question = change['new']\r\n",
        "        \r\n",
        "        def answer_que(questt, anss):\r\n",
        "          print(nlp({\r\n",
        "          'question': questt,\r\n",
        "          'context': anss\r\n",
        "      }))\r\n",
        "             \r\n",
        "        def transcriptName():\r\n",
        "\r\n",
        "          with open('data/uploaded_file.txt', 'r') as myfile: \r\n",
        "\r\n",
        "            next(myfile)\r\n",
        "            data=myfile.read().replace('\\n', '')\r\n",
        "\r\n",
        "            randomisation_que = answer_que(surgery_question, data)\r\n",
        "        \r\n",
        "        print('Computing...')\r\n",
        "\r\n",
        "        transcriptName()\r\n",
        "\r\n",
        "\r\n",
        "y.observe(on_change)\r\n",
        "\r\n",
        "display(y)\r\n",
        "\r\n",
        "\r\n",
        "z = widgets.Dropdown(\r\n",
        "    options=['What are the risks of surgery?' , 'What are the risks of operation?', 'Are there any risks?', 'What are the risks?'],\r\n",
        "    value=None,\r\n",
        "    description='Q4:',\r\n",
        ")\r\n",
        "\r\n",
        "\r\n",
        "def on_change(change):\r\n",
        "    if change['type'] == 'change' and change['name'] == 'value':\r\n",
        "        surgery_question = change['new']\r\n",
        "        \r\n",
        "        def answer_que(questt, anss):\r\n",
        "          print(nlp({\r\n",
        "          'question': questt,\r\n",
        "          'context': anss\r\n",
        "      }))\r\n",
        "             \r\n",
        "        def transcriptName():\r\n",
        "\r\n",
        "          with open('data/uploaded_file.txt', 'r') as myfile: \r\n",
        "\r\n",
        "            next(myfile)\r\n",
        "            data=myfile.read().replace('\\n', '')\r\n",
        "\r\n",
        "            randomisation_que = answer_que(surgery_question, data)\r\n",
        "        \r\n",
        "        print('Computing...')\r\n",
        "  \r\n",
        "        transcriptName()\r\n",
        "\r\n",
        "\r\n",
        "z.observe(on_change)\r\n",
        "\r\n",
        "display(z)\r\n",
        "\r\n",
        "\r\n",
        "g = widgets.Dropdown(\r\n",
        "    options=['Will you be receiving questionnaires?', 'What are the follow up procedures?', 'Will you follow up?', 'Will you fill the questonnaires?'],\r\n",
        "    value=None,\r\n",
        "    description='Q5:',\r\n",
        ")\r\n",
        "\r\n",
        "\r\n",
        "def on_change(change):\r\n",
        "    if change['type'] == 'change' and change['name'] == 'value':\r\n",
        "        surgery_question = change['new']\r\n",
        "        \r\n",
        "        def answer_que(questt, anss):\r\n",
        "          print(nlp({\r\n",
        "          'question': questt,\r\n",
        "          'context': anss\r\n",
        "      }))\r\n",
        "             \r\n",
        "        def transcriptName():\r\n",
        "\r\n",
        "          with open('data/uploaded_file.txt', 'r') as myfile: \r\n",
        "\r\n",
        "            next(myfile)\r\n",
        "            data=myfile.read().replace('\\n', '')\r\n",
        "\r\n",
        "            randomisation_que = answer_que(surgery_question, data)\r\n",
        "        \r\n",
        "        print('Computing...')\r\n",
        "  \r\n",
        "        transcriptName()\r\n",
        "\r\n",
        "\r\n",
        "g.observe(on_change)\r\n",
        "\r\n",
        "display(g)\r\n",
        "\r\n",
        "b = widgets.Dropdown(\r\n",
        "    options=['Are you happy with all that?', 'Are you happy to consent?', 'Are you ready to make a decision?', 'Which route do you want to go?', 'Do you want to go for surgery?', 'What do you want to go for?'],\r\n",
        "    value=None,\r\n",
        "    description='Q6:',\r\n",
        ")\r\n",
        "\r\n",
        "\r\n",
        "def on_change(change):\r\n",
        "    if change['type'] == 'change' and change['name'] == 'value':\r\n",
        "        surgery_question = change['new']\r\n",
        "        \r\n",
        "        def answer_que(questt, anss):\r\n",
        "          print(nlp({\r\n",
        "          'question': questt,\r\n",
        "          'context': anss\r\n",
        "      }))\r\n",
        "             \r\n",
        "        def transcriptName():\r\n",
        "\r\n",
        "          with open('data/uploaded_file.txt', 'r') as myfile: \r\n",
        "\r\n",
        "            next(myfile)\r\n",
        "            data=myfile.read().replace('\\n', '')\r\n",
        "\r\n",
        "            randomisation_que = answer_que(surgery_question, data)\r\n",
        "        \r\n",
        "        print('Computing...')\r\n",
        "  \r\n",
        "        transcriptName()\r\n",
        "\r\n",
        "\r\n",
        "b.observe(on_change)\r\n",
        "\r\n",
        "display(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOWEkvb8rhQy"
      },
      "source": [
        "## Step 5 - Check textual entailment, contradiction and neutrality\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_-gukiynGVe"
      },
      "source": [
        "#@title ⠀ {display-mode: \"form\"}\r\n",
        "\r\n",
        "x = predictor.predict(\r\n",
        "    premise=input('Premise: '), \r\n",
        "    hypothesis=input('Hypothesis: ')\r\n",
        ")\r\n",
        "\r\n",
        "x['label_probs'] # entailment, contradiciton, neutral"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}