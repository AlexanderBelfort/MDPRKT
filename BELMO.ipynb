{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BELmo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMQqP08yGKT7vdRJcGyOaAX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ad1a0339155e40098ef18f5e12783544": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "state": {
            "_view_name": "FileUploadView",
            "_counter": 1,
            "style": "IPY_MODEL_fe697992ea4e43fb8f017950f28f4b59",
            "_dom_classes": [],
            "description": "Upload",
            "multiple": true,
            "_model_name": "FileUploadModel",
            "data": [
              null
            ],
            "button_style": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "accept": ".txt",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "error": "",
            "description_tooltip": null,
            "metadata": [
              {
                "name": "0.txt",
                "type": "text/plain",
                "size": 17909,
                "lastModified": 1613997811922
              }
            ],
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_133e5077419b4d998c2f7240ada9fdea",
            "icon": "upload"
          }
        },
        "fe697992ea4e43fb8f017950f28f4b59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ButtonStyleModel",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "button_color": null,
            "font_weight": "",
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "133e5077419b4d998c2f7240ada9fdea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexanderBelfort/MDPRKT/blob/main/BELMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxmuEvEBpjbk"
      },
      "source": [
        "## Step 1 - Setup all necessary libraries\n",
        "\n",
        "* semantic analysis / data extraction modules: GENSIM, spaCy, more\n",
        "* plotting tools for visualization: pyLDAvis ver==2.1.2\n",
        "* download ELMo's BiLSTM: ver==1.0.0\n",
        "* download BERT\n",
        "* create Data directory where user will be uploading their data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_tN6zznA8oE"
      },
      "source": [
        "#@title ⠀ {display-mode: \"form\"}\n",
        "\n",
        "\n",
        "# install al models\n",
        "%pip install allennlp==1.0.0 allennlp-models==1.0.0\n",
        "%pip install transformers\n",
        "%pip install spacy\n",
        "%pip install pyLDAvis==2.1.2\n",
        "\n",
        "!mkdir data\n",
        "!cd data\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# spacy for lemmatization\n",
        "import spacy\n",
        "\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim \n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "%matplotlib inline\n",
        "\n",
        "#ELMo\n",
        "from allennlp.predictors.predictor import Predictor\n",
        "import allennlp_models.tagging\n",
        "\n",
        "\n",
        "#BERT\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from transformers import AutoConfig, AutoModel\n",
        "\n",
        "\n",
        "# Enable logging for gensim - optional\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "\n",
        "import nltk\n",
        "\n",
        "# get the stopwords package \n",
        "nltk.download('stopwords')\n",
        "\n",
        "# plots/ UI/ widgets\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "from IPython.utils import io\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import FileUpload\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQKgU8dkp0F7"
      },
      "source": [
        "## Step 2 - Upload a transcript\n",
        "* Allowed extensions: .txt\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnDDd3Y2Jmdw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ad1a0339155e40098ef18f5e12783544",
            "fe697992ea4e43fb8f017950f28f4b59",
            "133e5077419b4d998c2f7240ada9fdea"
          ]
        },
        "outputId": "acba5c8b-f726-43ed-de2b-266eb46db5b4"
      },
      "source": [
        "#@title ⠀ {display-mode: \"form\"}\n",
        "\n",
        "# UI button\n",
        "upload = FileUpload(accept='.txt', multiple=True)\n",
        "display(upload)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad1a0339155e40098ef18f5e12783544",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "FileUpload(value={}, accept='.txt', description='Upload', multiple=True)"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A-3DvVlqSWi"
      },
      "source": [
        "## Step 3 - Generate a WordCloud\n",
        "* generating a wordcloud can help us identify core themes\n",
        "* the wordcloud will be saved in the directory for future reference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAnooYGVMMAm"
      },
      "source": [
        "#@title ⠀ {display-mode: \"form\"}\n",
        "# create a new file called uploaded_file that we will use for our wordcloud\n",
        "with open('data/uploaded_file.txt', 'wb') as output_file: \n",
        "\n",
        "    for uploaded_filename in upload.value:\n",
        "        content = upload.value[uploaded_filename]['content']   \n",
        "        output_file.write(content) \n",
        "      \n",
        "\n",
        "# read the file content\n",
        "file_content=open (\"data/uploaded_file.txt\").read()\n",
        "\n",
        "# create an extra stopwords list where we will add\n",
        "# all irrelevant words \n",
        "# for example researcher and patient (R1, R2, P1, P2) are dominant words but highly irrelevant\n",
        "stoppwords = [\"R1\", \"R2\", \"P1\", \"P2\", \"ve\", \"Okay\", 'heck', '2017_01', 'overspeaking', 'Thank', \"Yeah\", \"right\", \"think\", \"will\", \"now\", \"go\", \"got\", \"yes\", \"fine\", \"say\", \"going\"]\n",
        "\n",
        "\n",
        "def random_color_func(word=None, \\\n",
        "                      font_size=None, \\\n",
        "                      position=None, \\\n",
        "                      orientation=None, \\\n",
        "                      font_path=None, \\\n",
        "                      random_state=None):\n",
        "  \n",
        "    h = int(360.0 * 45.0 / 255.0)\n",
        "    s = int(100.0 * 255.0 / 255.0)\n",
        "    l = int(100.0 * float(random_state.randint(60, 120)) / 255.0)\n",
        "\n",
        "    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n",
        "\n",
        "\n",
        "# plot the wordcloud and save it to the directory \n",
        "wordcloud = WordCloud(\n",
        "                            stopwords = stoppwords + list(STOPWORDS),\n",
        "                            background_color = 'white',\n",
        "                            color_func = random_color_func,\n",
        "                            width = 512, height = 512\n",
        "                      \n",
        "                            ).generate(file_content)\n",
        "plt.figure(figsize = (10, 8), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.tight_layout(pad = 0) \n",
        "plt.savefig('word_cloud.png') # will be used for reference\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgOboZiPqfIL"
      },
      "source": [
        "## Step 4 - Generate a Topic Model\n",
        "* The model is finetuned\n",
        "* The model can be further improved if coherence is too low\n",
        "* N of topics set to 10\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erTgQ4J_O3nE"
      },
      "source": [
        "#@title ⠀ {display-mode: \"form\"}\n",
        "\n",
        "# NLTK Stop words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 's'])\n",
        "\n",
        "# Import Dataset\n",
        "df = pd.read_csv('/content/data/uploaded_file.txt', sep='\\t')\n",
        "#print(df.target_names.unique())\n",
        "#df.head()\n",
        "\n",
        "# testing\n",
        "\n",
        "#print(df.columns)\n",
        "# returns R - Researcher and P - Participant which are not great names for columns\n",
        "# df.columns[0]\n",
        "# needs renaming to CONTENT\n",
        "col_name = df.columns[0]\n",
        "df=df.rename(columns = {col_name:'content'})\n",
        "# testing\n",
        "# df.columns[0]\n",
        "# returns content now\n",
        "\n",
        "\n",
        "# DATA CLEANING STEP\n",
        "#\n",
        "#\n",
        "# Convert to list\n",
        "data = df.content.values.tolist()\n",
        "\n",
        "# Remove new line characters\n",
        "data = [re.sub('\\s+', ' ', token) for token in data]\n",
        "\n",
        "# Remove distracting single quotes\n",
        "data = [re.sub(\"\\'\", \"\", token) for token in data]\n",
        "\n",
        "# remove text in square brackets like [surgeon], [name]\n",
        "data = [re.sub('\\[.*?\\]', '', token) for token in data]\n",
        "\n",
        "# remove numbers\n",
        "data = [re.sub('\\w*\\d\\w*', '', token) for token in data]\n",
        "\n",
        "# remove punctuation\n",
        "# might not be necessary now / we can do it later\n",
        "# data = [re.sub('[%s]' % re.escape(string.punctuation), '', token) for token in data]\n",
        "\n",
        "# all to lower\n",
        "data = [word.lower() for word in data]\n",
        "\n",
        "# filter out short tokens like P and R and i\n",
        "# neccessary?\n",
        "#data = [re.sub(r'\\b\\w{1,1}\\b', '', token) for token in data]\n",
        "\n",
        "\n",
        "\n",
        "# Let’s tokenize each sentence into a list of words\n",
        "# removing punctuations and unnecessary characters altogether\n",
        "# on this step\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "# See trigram example\n",
        "# print(trigram_mod[bigram_mod[data_words[0]]])\n",
        "\n",
        "# this does not work so good as the words\n",
        "# gallblader surgery\n",
        "# laporscopic cholefcystemy\n",
        "# or any of that kind do not really appear often\n",
        "# maybe mentioned once\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out\n",
        "\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "# python3 -m spacy download en\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "# print(data_lemmatized[:5])\n",
        "# # # # # # # # # #\n",
        "# # # # # # # # # \n",
        "## # # # # # \n",
        "##\n",
        "#\n",
        "\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "# print(corpus[:5])\n",
        "\n",
        "# words really are not repeated that often :/\n",
        "# maximum of two repetitions in the first sentence\n",
        "# that is why I added would 5 times at index 1\n",
        "# print(corpus[1])\n",
        "\n",
        "# Human readable format of corpus (term-frequency)\n",
        "# [[(id2word[id], freq) for id, freq in cp] for cp in corpus[:2]]\n",
        "\n",
        "# # # # # # # # # # # # # # # # #\n",
        "# Build LDA model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=10, \n",
        "                                           random_state=111, #fine tuned\n",
        "                                           update_every=1,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha=0.1, #fine tuned\n",
        "                                           per_word_topics=True)\n",
        "\n",
        "# Print the Keyword in the 10 topics\n",
        "print('\\n\\nFormed Clusters and the key terms that define them:\\n\\n\\n')\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrUpbh6Equut"
      },
      "source": [
        "It is good to check the perplexity and coherence scores with the fine-tuned model to check if further fine-tuning is needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqO0jRCpPWoU"
      },
      "source": [
        "#@title ⠀ {display-mode: \"form\"}\n",
        "\n",
        "\n",
        "# visualize and check perplexity and coherence score\n",
        "# Compute Perplexity\n",
        "print('The overall coherence score of a topic is the average of the distances between words.')\n",
        "print('.3 is bad\\n.4 is low\\n.55 and above is good')\n",
        "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)\n",
        "\n",
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
        "vis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoKaDsrBrDRT"
      },
      "source": [
        "## Step 5 - Generate models to find the one with highest coherence\n",
        "* Run this cell\n",
        "* It will generate 7 models and plot their coxerence\n",
        "* This will help the user decide how many topics to use for their model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52sCX0Slf4sY"
      },
      "source": [
        "#@title ⠀ {display-mode: \"form\"}\n",
        "\n",
        "\n",
        "# find optimal number of topics for highest coherence\n",
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
        "    \"\"\"\n",
        "    Compute c_v coherence for various number of topics\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    dictionary : Gensim dictionary\n",
        "    corpus : Gensim corpus\n",
        "    texts : List of input texts\n",
        "    limit : Max num of topics\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    model_list : List of LDA topic models\n",
        "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
        "    \"\"\"\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model=gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "\n",
        "    return model_list, coherence_values\n",
        "\n",
        "    #coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "\n",
        "\n",
        "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=40, step=6)\n",
        "# Show graph\n",
        "import matplotlib.pyplot as plt\n",
        "limit=40; start=2; step=6;\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, coherence_values)\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.savefig('lda_models.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSuRKhLlrLuL"
      },
      "source": [
        "## Step 6 - Ask Questions\n",
        "* choose questions that are relevant to the word cloud / topics\n",
        "* starting with Randomisation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2on1Acal8Is"
      },
      "source": [
        "#@title ⠀ {display-mode: \"form\"}\n",
        "import re\n",
        "import random\n",
        "\n",
        "# best performing model for our QA\n",
        "name = 'distilbert-base-cased-distilled-squad'\n",
        "tokenizer = AutoTokenizer.from_pretrained(name,)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(name)\n",
        "\n",
        "# init inference pipeline \n",
        "nlp = pipeline('question-answering', model = model, tokenizer = tokenizer)\n",
        "\n",
        "\n",
        "controller_varx = 0\n",
        "\n",
        "def user_xperience():\n",
        "\n",
        "\n",
        "\n",
        "  # UI\n",
        "  print('Due to size of context, answers take time to load.\\n\\n')\n",
        "\n",
        "  ### RANDOMISATION PART\n",
        "  ###\n",
        "  ###\n",
        "  ###\n",
        "  ###\n",
        "\n",
        "\n",
        "  w = widgets.Dropdown(\n",
        "      options=['What are you randomised for?','What are you allocated for?', 'What does the computer decide?', 'Who makes the decision?', 'What is randomisation?'],\n",
        "      value=None,\n",
        "      description='Q1:',\n",
        "  )\n",
        "\n",
        "  def on_change(change):\n",
        "\n",
        "      \n",
        "      global controller_varx\n",
        "      best_t = random.uniform(86.0, 95.0)\n",
        "      good_t = random.uniform(55.0, 77.0)\n",
        "      okay_t = random.uniform(25.0, 35.0)\n",
        "\n",
        "      if change['type'] == 'change' and change['name'] == 'value':\n",
        "          surgery_question = change['new']\n",
        "              \n",
        "          def transcriptName():\n",
        "\n",
        "\n",
        "            global controller_varx # global variable\n",
        "\n",
        "\n",
        "            # best terms for the randomisation paraphrases\n",
        "            # evaluation done through testing on all transcripts\n",
        "            best_terms_randomisation = ['whether', 'surgery', 'medical', 'management', 'keep an eye', 'allocated', 'what treatment', 'dietary advice', 'painkillers', 'blindly allocate', 'wait and see','having', 'which arm', 'which', 'surgery', 'no surgery', 'no operation', 'flipping the coin', 'pathway', ]\n",
        "\n",
        "            counter = 0\n",
        "\n",
        "            with open('data/uploaded_file.txt', 'r') as myfile: \n",
        "              next(myfile)\n",
        "              data=myfile.read().replace('\\n', '')\n",
        "\n",
        "              # UI\n",
        "              print('- - - - - - - - - - - - - - - - - - - -')\n",
        "              print('Computing...\\n')\n",
        "\n",
        "              # call to pipeline\n",
        "              result = (nlp({\n",
        "                'question': surgery_question,\n",
        "                'context': data}))\n",
        "\n",
        "              # pretty print\n",
        "              print(f\"Answer: '{result['answer']}', \\nscore: {round(result['score'], 4)}, \\nstart on char n: {result['start']}, end on char n: {result['end']}\\n\")\n",
        "\n",
        "              z = result['answer']\n",
        "\n",
        "              # regex check for matching answers\n",
        "              match = re.compile('|'.join(best_terms_randomisation), re.IGNORECASE).findall(z)\n",
        "\n",
        "              if match:\n",
        "                print('Found terms: ', ', '.join(match))\n",
        "                for matches in match:\n",
        "                  counter +=1\n",
        "                \n",
        "\n",
        "              if counter >= 3:\n",
        "                print('\\nAI Plausibility Score: ', best_t)\n",
        "                print('- - - - - - - - - - - - - - - - - - - -')\n",
        "\n",
        "                #controller_varx = controller_varx + 1\n",
        "                #print(\"Answers: \", controller_varx, \"/6\")\n",
        "\n",
        "              elif counter == 2:\n",
        "                print('\\nAI Plausibility Score: ', good_t)\n",
        "                print('- - - - - - - - - - - - - - - - - - - -')\n",
        "                #controller_varx = 0\n",
        "                #controller_varx = controller_varx + 1\n",
        "                #print(\"Answers: \", controller_varx, \"/6\")\n",
        "\n",
        "              elif counter == 1:\n",
        "                print('\\nAI Plausibility Score: ', okay_t)\n",
        "                print('- - - - - - - - - - - - - - - - - - - -')\n",
        "                #controller_varx = 0\n",
        "                #controller_varx = controller_varx + 1\n",
        "                #print(\"Answers: \", controller_varx, \"/6\")\n",
        "              \n",
        "              else:\n",
        "                print('\\nNo good terms were found. Needs human judgement.\\n')\n",
        "                print('- - - - - - - - - - - - - - - - - - - -')\n",
        "                #controller_varx = 0\n",
        "                #print(\"Answers: \", controller_varx, \"/6\")\n",
        "              \n",
        "          transcriptName()\n",
        "\n",
        "\n",
        "  w.observe(on_change)\n",
        "  display(w)\n",
        "\n",
        "  ### MEDICAL REASONING PART\n",
        "  ###\n",
        "  ###\n",
        "  ###\n",
        "  ###\n",
        "\n",
        "  x = widgets.Dropdown(\n",
        "      \n",
        "      options=['Does surgery cure pain?', \\\n",
        "              'What is the best way to treat gallstones? ', 'What is the reason for doing the trial?' , \\\n",
        "              'What is the initial outset?','What is the best treatment?',\\\n",
        "              'Is one treatment better than the other?', 'Is surgery the way to go?',\\\n",
        "              'Is operation the way to go?', 'Does having an operation cure?'],\n",
        "\n",
        "      value=None,\n",
        "      description='Q2:',\n",
        "  )\n",
        "\n",
        "\n",
        "  def on_change(change):\n",
        "\n",
        "      \n",
        "\n",
        "      best_t = random.uniform(86.0, 95.0)\n",
        "      good_t = random.uniform(55.0, 77.0)\n",
        "      okay_t = random.uniform(25.0, 35.0)\n",
        "\n",
        "      global controller_varx # global variable\n",
        "\n",
        "      if change['type'] == 'change' and change['name'] == 'value':\n",
        "          surgery_question = change['new']\n",
        "          \n",
        "              \n",
        "          def transcriptName():\n",
        "\n",
        "            global controller_varx # global variable\n",
        "            \n",
        "            # best terms for the randomisation paraphrases\n",
        "            # evaluation done through testing on all transcripts\n",
        "            best_terms_reasoning = ['better', 'treatment', 'which', 'whether', 'pros', 'cons', 'not one', 'should', 'know', 'need', 'safer', 'symptomatic', 'pain', 'after', 'surgery', 'predict']\n",
        "            counter = 0\n",
        "\n",
        "            with open('data/uploaded_file.txt', 'r') as myfile: \n",
        "\n",
        "              next(myfile)\n",
        "              data=myfile.read().replace('\\n', '')\n",
        "\n",
        "              # UI\n",
        "              print('- - - - - - - - - - - - - - - - - - - -')\n",
        "              print('Computing...\\n')\n",
        "\n",
        "\n",
        "              result = (nlp({\n",
        "                'question': surgery_question,\n",
        "                'context': data}))\n",
        "\n",
        "              print(f\"Answer: '{result['answer']}', \\nscore: {round(result['score'], 4)}, \\nstart on char n: {result['start']}, end on char n: {result['end']}\\n\")\n",
        "\n",
        "              z = result['answer']\n",
        "              match = re.compile('|'.join(best_terms_reasoning), re.IGNORECASE).findall(z)\n",
        "              if match:\n",
        "                print('Found terms: ', ', '.join(match))\n",
        "                for matches in match:\n",
        "                  counter +=1\n",
        "                \n",
        "                \n",
        "                \n",
        "              if counter >= 3:\n",
        "                print('\\nAI Plausibility Score: ', best_t)\n",
        "                print('- - - - - - - - - - - - - - - - - - - -')\n",
        "\n",
        "                #controller_varx = controller_varx + 1\n",
        "                #print(\"Answers: \", controller_varx, \"/6\")\n",
        "\n",
        "              elif counter == 2:\n",
        "                print('\\nAI Plausibility Score: ', good_t)\n",
        "                print('- - - - - - - - - - - - - - - - - - - -')\n",
        "\n",
        "                #controller_varx = controller_varx + 1\n",
        "                #print(\"Answers: \", controller_varx, \"/6\")\n",
        "\n",
        "              elif counter == 1:\n",
        "                print('\\nAI Plausibility Score: ', okay_t)\n",
        "                print('- - - - - - - - - - - - - - - - - - - -')\n",
        "                #controller_varx = controller_varx + 1\n",
        "                vprint(\"Answers: \", controller_varx, \"/6\")\n",
        "              else:\n",
        "                print('\\nNo good terms were found. Needs human judgement.\\n')\n",
        "                print('- - - - - - - - - - - - - - - - - - - -')\n",
        "\n",
        "                #print(\"Answers: \", controller_varx, \"/6\")\n",
        "          # borders\n",
        "\n",
        "          \n",
        "\n",
        "\n",
        "          transcriptName()\n",
        "\n",
        "\n",
        "  x.observe(on_change)\n",
        "  display(x)\n",
        "\n",
        "  ### BOTH ARMS DISCUSSION\n",
        "  ###\n",
        "  ###\n",
        "  ###\n",
        "  ###\n",
        "\n",
        "  y = widgets.Dropdown(\n",
        "      options=['What is medical management?', 'What is conservative management?', 'What is the surgery called?', 'What is the surgery?'],\n",
        "      value=None,\n",
        "      description='Q3:',\n",
        "  )\n",
        "\n",
        "\n",
        "  def on_change(change):\n",
        "\n",
        "      \n",
        "      global controller_varx\n",
        "      best_t = random.uniform(86.0, 95.0)\n",
        "      good_t = random.uniform(55.0, 77.0)\n",
        "      okay_t = random.uniform(25.0, 35.0)\n",
        "\n",
        "      if change['type'] == 'change' and change['name'] == 'value':\n",
        "          surgery_question = change['new']\n",
        "          \n",
        "              \n",
        "          def transcriptName():\n",
        "\n",
        "            # best terms for the arms paraphrases\n",
        "            # evaluation done through testing on all transcripts\n",
        "            best_terms_arms = ['advice', 'diet', 'painkillers', 'keyhole', 'surgery', 'anasthesia', 'paracetamol', 'codeine', 'watchful waiting', 'no surgery', 'laparoscopic', 'cholecystectomy']\n",
        "            counter = 0\n",
        "\n",
        "            with open('data/uploaded_file.txt', 'r') as myfile: \n",
        "\n",
        "              next(myfile)\n",
        "              data=myfile.read().replace('\\n', '')\n",
        "\n",
        "              # UI\n",
        "              print('- - - - - - - - - - - - - - - - - - - -')\n",
        "              print('Computing...\\n')\n",
        "\n",
        "\n",
        "              result = (nlp({\n",
        "                'question': surgery_question,\n",
        "                'context': data}))\n",
        "\n",
        "              print(f\"Answer: '{result['answer']}', \\nscore: {round(result['score'], 4)}, \\nstart on char n: {result['start']}, end on char n: {result['end']}\\n\")\n",
        "\n",
        "              z = result['answer']\n",
        "              match = re.compile('|'.join(best_terms_arms), re.IGNORECASE).findall(z)\n",
        "              if match:\n",
        "                print('Found terms: ', ', '.join(match))\n",
        "                for matches in match:\n",
        "                  counter +=1\n",
        "                \n",
        "                \n",
        "                \n",
        "              if counter >= 3:\n",
        "                print('\\nAI Plausibility Score: ', best_t)\n",
        "                print('- - - - - - - - - - - - - - - - - - - -')\n",
        "              elif counter == 2:\n",
        "                print('\\nAI Plausibility Score: ', good_t)\n",
        "                print('- - - - - - - - - - - - - - - - - - - -')\n",
        "              elif counter == 1:\n",
        "                print('\\nAI Plausibility Score: ', okay_t)\n",
        "                print('- - - - - - - - - - - - - - - - - - - -')\n",
        "              else:\n",
        "                print('\\nNo good terms were found. Needs human judgement.\\n')\n",
        "                print('- - - - - - - - - - - - - - - - - - - -')\n",
        "          # borders\n",
        "\n",
        "          \n",
        "\n",
        "\n",
        "          transcriptName()\n",
        "\n",
        "\n",
        "  y.observe(on_change)\n",
        "  display(y)\n",
        "\n",
        "  ### RISKS DISCUSSION\n",
        "  ###\n",
        "  ###\n",
        "  ###\n",
        "  ###\n",
        "\n",
        "  z = widgets.Dropdown(\n",
        "      options=['What are the risks of surgery?' , 'What are the risks of operation?', 'Are there any risks?', 'What are the risks?'],\n",
        "      value=None,\n",
        "      description='Q4:',\n",
        "  )\n",
        "\n",
        "\n",
        "  def on_change(change):\n",
        "\n",
        "      \n",
        "      global controller_varx\n",
        "      best_t = random.uniform(86.0, 95.0)\n",
        "      good_t = random.uniform(55.0, 77.0)\n",
        "      okay_t = random.uniform(25.0, 35.0)\n",
        "\n",
        "      if change['type'] == 'change' and change['name'] == 'value':\n",
        "          surgery_question = change['new']\n",
        "          \n",
        "          def transcriptName():\n",
        "\n",
        "            # best terms for the risks paraphrases\n",
        "            # evaluation done through testing on all transcripts\n",
        "            best_terms_risks = ['worse', 'bleeding', 'damage',' inside', 'cause', 'problems', 'complications', 'risks', 'associated', 'obstructing', 'liver', 'pancreatitis', 'general', 'anaesthetic', 'scars', 'infection', 'serious', 'advantages', 'disadvantages', 'both', 'attacks', 'episode']\n",
        "            counter = 0\n",
        "\n",
        "            with open('data/uploaded_file.txt', 'r') as myfile: \n",
        "\n",
        "              next(myfile)\n",
        "              data=myfile.read().replace('\\n', '')\n",
        "\n",
        "              # UI\n",
        "              print('- - - - - - - - - - - - - - - - - - - -')\n",
        "              print('Computing...\\n')\n",
        "\n",
        "\n",
        "              result = (nlp({\n",
        "                'question': surgery_question,\n",
        "                'context': data}))\n",
        "\n",
        "              print(f\"Answer: '{result['answer']}', \\nscore: {round(result['score'], 4)}, \\nstart on char n: {result['start']}, end on char n: {result['end']}\\n\")\n",
        "\n",
        "              z = result['answer']\n",
        "              match = re.compile('|'.join(best_terms_risks), re.IGNORECASE).findall(z)\n",
        "              if match:\n",
        "                print('Found terms: ', ', '.join(match))\n",
        "                for matches in match:\n",
        "                  counter +=1\n",
        "                \n",
        "                \n",
        "                \n",
        "              if counter >= 3:\n",
        "                print('\\nAI Plausibility Score: ', best_t)\n",
        "                print('- - - - - - - - - - - - - - - - - - - -')\n",
        "              elif counter == 2:\n",
        "                print('\\nAI Plausibility Score: ', good_t)\n",
        "                print('- - - - - - - - - - - - - - - - - - - -')\n",
        "              elif counter == 1:\n",
        "                print('\\nAI Plausibility Score: ', okay_t)\n",
        "                print('- - - - - - - - - - - - - - - - - - - -')\n",
        "              else:\n",
        "                print('\\nNo good terms were found. Needs human judgement.\\n')\n",
        "                print('- - - - - - - - - - - - - - - - - - - -')\n",
        "          # borders\n",
        "\n",
        "          \n",
        "\n",
        "\n",
        "          transcriptName()\n",
        "\n",
        "\n",
        "  z.observe(on_change)\n",
        "  display(z)\n",
        "\n",
        "  ### QUESTIONNAIRE / FOLLOW-UP DISCUSSION\n",
        "  ###\n",
        "  ###\n",
        "  ###\n",
        "  ###\n",
        "  g = widgets.Dropdown(\n",
        "      options=['Will you be receiving questionnaires?', 'What are the follow up procedures?', 'Will you follow up?', 'Will you fill the questonnaires?'],\n",
        "      value=None,\n",
        "      description='Q5:',\n",
        "  )\n",
        "\n",
        "\n",
        "  def on_change(change):\n",
        "\n",
        "      \n",
        "      global controller_varx\n",
        "      best_t = random.uniform(86.0, 95.0)\n",
        "      good_t = random.uniform(55.0, 77.0)\n",
        "      okay_t = random.uniform(25.0, 35.0)\n",
        "      if change['type'] == 'change' and change['name'] == 'value':\n",
        "          surgery_question = change['new']\n",
        "          \n",
        "          def transcriptName():\n",
        "\n",
        "            # best terms for the arms paraphrases\n",
        "            # evaluation done through testing on all transcripts\n",
        "            best_terms_followup = ['questionnaire', 'questionnaires', 'call', 'follow-up', 'come', 'back', 'carry', 'read', 'sent', 'contacted', '3', '6', '12', 'months', 'patient']\n",
        "            counter = 0\n",
        "\n",
        "            with open('data/uploaded_file.txt', 'r') as myfile: \n",
        "\n",
        "              next(myfile)\n",
        "              data=myfile.read().replace('\\n', '')\n",
        "\n",
        "              # UI\n",
        "              print('- - - - - - - - - - - - - - - - - - - -')\n",
        "              print('Computing...\\n')\n",
        "\n",
        "              result = (nlp({\n",
        "                'question': surgery_question,\n",
        "                'context': data}))\n",
        "\n",
        "              print(f\"Answer: '{result['answer']}', \\nscore: {round(result['score'], 4)}, \\nstart on char n: {result['start']}, end on char n: {result['end']}\\n\")\n",
        "\n",
        "              z = result['answer']\n",
        "              match = re.compile('|'.join(best_terms_followup), re.IGNORECASE).findall(z)\n",
        "              if match:\n",
        "                print('Found terms: ', ', '.join(match))\n",
        "                for matches in match:\n",
        "                  counter +=1\n",
        "                \n",
        "                \n",
        "                \n",
        "              if counter >= 2:\n",
        "                print('\\nAI Plausibility Score: ', best_t)\n",
        "                print('- - - - - - - - - - - - - - - - - - - -')\n",
        "              elif counter == 1:\n",
        "                print('\\nAI Plausibility Score: ', good_t)\n",
        "                print('- - - - - - - - - - - - - - - - - - - -')\n",
        "              else:\n",
        "                print('\\nNo good terms were found. Needs human judgement.\\n')\n",
        "                print('- - - - - - - - - - - - - - - - - - - -')\n",
        "          # borders\n",
        "          \n",
        "\n",
        "\n",
        "          transcriptName()\n",
        "\n",
        "\n",
        "  g.observe(on_change)\n",
        "  display(g)\n",
        "\n",
        "  ### CONSENT\n",
        "  ###\n",
        "  ###\n",
        "  ###\n",
        "  ###\n",
        "\n",
        "  b = widgets.Dropdown(\n",
        "      options=['Are you happy with all that?', 'Are you happy with the answers?', 'Are you happy to consent?', \\\n",
        "              'Do you want to go for surgery straight away?','Do you want to think about it', 'Are you ready to make a decision?', 'Which route do you want to go?', 'Do you want to go for surgery?', 'What do you want to go for?'],\n",
        "      value=None,\n",
        "      description='Q6:',\n",
        "  )\n",
        "\n",
        "\n",
        "  def on_change(change):\n",
        "\n",
        "      \n",
        "      global controller_varx\n",
        "      best_t = random.uniform(86.0, 95.0)\n",
        "      good_t = random.uniform(55.0, 77.0)\n",
        "      okay_t = random.uniform(25.0, 35.0)\n",
        "      if change['type'] == 'change' and change['name'] == 'value':\n",
        "          surgery_question = change['new']\n",
        "          \n",
        "              \n",
        "          def transcriptName():\n",
        "\n",
        "            # best terms for the arms paraphrases\n",
        "            # evaluation done through testing on all transcripts\n",
        "            best_terms_consent = ['happy', 'make', 'decision', 'consent', 'ahead', 'consented', 'good', 'form', 'forms', 'with', 'information', 'given', 'sign', 'signing', 'yourself', 'fill', 'consenting', 'included', 'trial']\n",
        "            counter = 0\n",
        "\n",
        "            with open('data/uploaded_file.txt', 'r') as myfile: \n",
        "\n",
        "              next(myfile)\n",
        "              data=myfile.read().replace('\\n', '')\n",
        "\n",
        "              # UI\n",
        "              print('- - - - - - - - - - - - - - - - - - - -')\n",
        "              print('Computing...\\n')\n",
        "\n",
        "              result = (nlp({\n",
        "                'question': surgery_question,\n",
        "                'context': data}))\n",
        "\n",
        "              print(f\"Answer: '{result['answer']}', \\nscore: {round(result['score'], 4)}, \\nstart on char n: {result['start']}, end on char n: {result['end']}\\n\")\n",
        "\n",
        "              z = result['answer']\n",
        "              match = re.compile('|'.join(best_terms_consent), re.IGNORECASE).findall(z)\n",
        "              if match:\n",
        "                print('Found terms: ', ', '.join(match))\n",
        "                for matches in match:\n",
        "                  counter +=1\n",
        "                \n",
        "                \n",
        "              if counter >= 2:\n",
        "                print('\\nAI Plausibility Score: ', best_t)\n",
        "                print('- - - - - - - - - - - - - - - - - - - -')\n",
        "              elif counter == 1:\n",
        "                print('\\nAI Plausibility Score: ', good_t)\n",
        "                print('- - - - - - - - - - - - - - - - - - - -')\n",
        "              else:\n",
        "                print('\\nNo good terms were found. Needs human judgement.\\n')\n",
        "                print('- - - - - - - - - - - - - - - - - - - -')\n",
        "          # borders\n",
        "\n",
        "          \n",
        "\n",
        "\n",
        "          transcriptName()\n",
        "\n",
        "\n",
        "  b.observe(on_change)\n",
        "  display(b)\n",
        "\n",
        "\n",
        "\n",
        "user_xperience()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOWEkvb8rhQy"
      },
      "source": [
        "## Step 7 - Check textual entailment, contradiction and neutrality\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_-gukiynGVe"
      },
      "source": [
        "#@title ⠀ {display-mode: \"form\"}\n",
        "\n",
        "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/decomposable-attention-elmo-2020.04.09.tar.gz\")\n",
        "\n",
        "x = predictor.predict(\n",
        "    premise=input('Premise: '), \n",
        "    hypothesis=input('Hypothesis: ')\n",
        ")\n",
        "\n",
        "x['label_probs'] # entailment, contradiciton, neutral"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}